name: scrape-pubmed-single-neuron

on:
  workflow_dispatch:

permissions:
  contents: write  # needed so the workflow can commit & push data/

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Cache paperscraper dumps
        uses: actions/cache@v4
        with:
          path: server_dumps
          key: paperscraper-dumps-v1

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Download biorxiv dump if missing
        run: |
          python - <<'PY'
          from pathlib import Path
          from paperscraper.get_dumps import biorxiv

          dump = Path("server_dumps") / "biorxiv_metadata.jsonl"
          if dump.exists() and dump.stat().st_size > 0:
              print(f"biorxiv dump already present ({dump.stat().st_size/1e6:.1f} MB)")
          else:
              dump.parent.mkdir(parents=True, exist_ok=True)
              print("Downloading biorxiv dump (first run can take several minutes)...")
              biorxiv()
              print("biorxiv dump downloaded.")
          PY

      - name: Run PubMed scraper
        run: |
          python pubmed_single_neuron_scrape.py

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          # Only commit if there are changes
          git diff --cached --quiet || git commit -m "Update PubMed single-neuron dump [skip ci]"
          git push
